{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv \n",
    "from textblob import TextBlob\n",
    "from ast import literal_eval\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Dataset Australian_User_reviews.json\n",
    "    Contiene datos de las recomendaciones y comentarios de los jugadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cargamos los datos desde la carpeta Datasets\n",
    "##Obserbamos que el Dataset tiene columnas con datos anidados en forma de diccionarios por lo que\n",
    "##deberemos tratarlo linea por linea\n",
    "\n",
    "# Creamos esta funcion para corregir y cargar cada linea del archivo 'australian_user_reviews.json' original\n",
    "def cargar_y_corregir_linea(linea):\n",
    "    # reemplaza las comillas simples con dobles y corrige valores booleanos\n",
    "    linea_corregida = linea.replace(\"'\", '\"').replace('True', 'true').replace('False', 'false')\n",
    "    return json.loads(linea_corregida)\n",
    "\n",
    "# lista para almacenar los datos desanidados\n",
    "data_list = []\n",
    "\n",
    "# extraemos números de la columna 'funny'\n",
    "numero_regex = re.compile(r'\\d+')\n",
    "\n",
    "# lee el archivo 'australian_user_reviews.json' original, corrige y procesa cada línea\n",
    "with open('./Datasets/australian_user_reviews.json', 'r', encoding='utf-8') as archivo_json:\n",
    "    for linea in archivo_json:\n",
    "        try:\n",
    "            entrada = cargar_y_corregir_linea(linea)\n",
    "            user_id = entrada['user_id']\n",
    "            user_url = entrada['user_url']\n",
    "            # iteramos sobre cada reseña\n",
    "            for reseña in entrada['reviews']:\n",
    "                # estraemos el numero de la columna 'funny'\n",
    "                funny_valor = re.search(numero_regex, reseña.get('funny', ''))\n",
    "                if funny_valor:\n",
    "                    funny = int(funny_valor.group())\n",
    "                else:\n",
    "                    funny = None\n",
    "                \n",
    "                # elimina 'Posted' de la columna 'posted'\n",
    "                posted = reseña.get('posted', '').replace('Posted ', '', 1) \n",
    "\n",
    "                reseña_dict = {\n",
    "                    'user_id': user_id,\n",
    "                    'user_url': user_url,\n",
    "                    'funny': funny,\n",
    "                    'posted': posted,\n",
    "                    'item_id': int(reseña.get('item_id', '')),  # convierte a entero\n",
    "                    'helpful': reseña.get('helpful', ''),\n",
    "                    'recommend': reseña.get('recommend', ''),\n",
    "                    'review': reseña.get('review', '')  # mantiene el texto original de la review\n",
    "                }\n",
    "                data_list.append(reseña_dict)\n",
    "        except json.JSONDecodeError as e:\n",
    "            None\n",
    "\n",
    "# crea el DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplaza los valores NaN por None en el DataFrame\n",
    "df = df.where(df.notna(), None)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La columna ['posted'] presenta formato de fecha completo y algunas sin el año, \n",
    "# deduzco que el año corresponde al año 2016 en curzo en el momento de la extraccion de los datos\n",
    "#por lo que realizaremos la extraccion, transformacion y asignacion del año correcto \n",
    "\n",
    "# Definimos una función para extraer y convertir la fecha\n",
    "def extract_date(posted_str):\n",
    "    if not isinstance(posted_str, str):\n",
    "        return None\n",
    "    \n",
    "    # Elimina el punto y espacios en blanco al principio y al final\n",
    "    date_str = posted_str.replace('.', '').strip()\n",
    "    try:\n",
    "        # Intenta convertir la cadena de fecha en un objeto de fecha utilizando datetime.strptime\n",
    "        date_obj = datetime.strptime(date_str, '%B %d, %Y').date()\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # Intenta nuevamente con el formato '%B %d' si la conversión falla\n",
    "            date_obj = datetime.strptime(date_str, '%B %d').date()\n",
    "            # Si solo tiene mes y día y no se proporcionó el año, asumimos el año 2016\n",
    "            date_obj = date_obj.replace(year=2016)\n",
    "        except ValueError:\n",
    "            # Si no se puede convertir a ninguna de las dos formas, devuelve None\n",
    "            date_obj = None\n",
    "\n",
    "    return date_obj\n",
    "\n",
    "# Aplica la función extract_date a la columna 'posted'\n",
    "df['posted'] = df['posted'].apply(extract_date)\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna 'user_url' porque no nos aporta informacion necesaria\n",
    "df.drop('user_url', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna 'helpful' porque no nos aporta informacion necesaria\n",
    "df.drop('helpful', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna 'funny' porque no nos aporta informacion necesaria\n",
    "df.drop('funny', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a- Analisis de Sentimientos\n",
    "    realizamos el analisis y adjuntamos la nueva columna al Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una funcion que nos permita analizar el sentimiento de las reseñas junto con su \n",
    "# recomendación. Esta función evaluará si una reseña es positiva, neutral o negativa, \n",
    "# considerando también si la recomendación asociada es verdadera (positiva) o falsa (negativa). \n",
    "# Luego, aplicaremos esta función a cada reseña en nuestro DataFrame para obtener \n",
    "# un análisis de sentimiento más completo.\n",
    "\n",
    "def get_sentiment_with_recommend(recommend, review):\n",
    "    if isinstance(review, str):  # Verifica si el tipo de review es string\n",
    "        if recommend:  # Verifica si la recomendación es verdadera\n",
    "            analysis = TextBlob(str(review))  # Convierte a cadena y analiza el texto de la reseña\n",
    "            polarity = analysis.sentiment.polarity\n",
    "            if polarity > 0:\n",
    "                return 2  # Positivo si polaridad > 0\n",
    "            else:\n",
    "                return 1  # Neutral si polaridad <= 0 y recommend true\n",
    "        else:\n",
    "            return 0  # No recomendado, se asume neutral o negativo\n",
    "    else:\n",
    "        return 1  # No hay reseña, se asume neutral\n",
    "\n",
    "# Convertimos la columna 'review' a cadena (str)\n",
    "df['review'] = df['review'].astype(str)\n",
    "\n",
    "# Aplicamos la función a cada review\n",
    "df['sentiment_analysis'] = df.apply(lambda x: get_sentiment_with_recommend(x['recommend'], x['review']), axis=1)\n",
    "\n",
    "# Reemplazamos la columna 'review' con la nueva columna 'sentiment_analysis'\n",
    "df.drop('review', axis=1, inplace=True)\n",
    "# Renombramos a la columna 'sentiment_analysis' como 'review' \n",
    "df.rename(columns={'sentiment_analysis': 'review'}, inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el df resultante en formato parquet \n",
    "df.to_parquet('./Datasets/user_reviews.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Dataset Australian_User_items.json\n",
    "    Contiene datos de Usuarios como Items y Tiempo de Juego "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cargamos los datos desde la carpeta Datasets\n",
    "##Obserbamos que el Dataset tiene columnas con datos anidados en forma de diccionarios por lo que\n",
    "##deberemos tratarlo linea por linea\n",
    "## Las Lineas con Errores de formato no se cargan en el df (55010 lineas)\n",
    "\n",
    "# Lista para almacenar los datos desanidados\n",
    "data_list = []\n",
    "\n",
    "# Función para corregir y cargar cada línea del archivo 'australian_users_items.json' original\n",
    "def cargar_y_corregir_linea(linea):\n",
    "    # reemplaza las comillas simples con dobles\n",
    "    linea_corregida = linea.replace(\"'\", '\"')\n",
    "    return json.loads(linea_corregida)\n",
    "\n",
    "# Crea y abre un archivo 'australian_users_items.csv' para escribir los datos\n",
    "with open('australian_users_items.csv', 'w', newline='', encoding='utf-8') as archivo_csv:\n",
    "    writer = csv.writer(archivo_csv)\n",
    "    \n",
    "    # Escribimos los nombres de las columnas\n",
    "    writer.writerow(['user_id', 'user_url', 'item_id', 'item_name', 'playtime_forever', 'playtime_2weeks'])\n",
    "    \n",
    "    # Inicializamos el contador de errores\n",
    "    errores_count = 0\n",
    "\n",
    "    # Lee el archivo 'australian_users_items.json' original, corrige y procesa cada línea\n",
    "    with open('./Datasets/australian_users_items.json', 'r', encoding='utf-8') as archivo_json:\n",
    "        for linea in archivo_json:\n",
    "            try:\n",
    "                entrada = cargar_y_corregir_linea(linea)\n",
    "                user_id = entrada['user_id']\n",
    "                user_url = entrada['user_url']\n",
    "                # iteracion sobre cada items\n",
    "                for articulo in entrada['items']:\n",
    "                    writer.writerow([\n",
    "                        user_id,\n",
    "                        user_url,\n",
    "                        articulo.get('item_id', ''),\n",
    "                        articulo.get('item_name', ''),\n",
    "                        articulo.get('playtime_forever', ''),\n",
    "                        articulo.get('playtime_2weeks', ''),\n",
    "                    ])\n",
    "                    data_list.append({\n",
    "                        'user_id': user_id,\n",
    "                        'user_url': user_url,\n",
    "                        'item_id': articulo.get('item_id', ''),\n",
    "                        'item_name': articulo.get('item_name', ''),\n",
    "                        'playtime_forever': articulo.get('playtime_forever', ''),\n",
    "                        'playtime_2weeks': articulo.get('playtime_2weeks', ''),\n",
    "                    })\n",
    "            except json.JSONDecodeError as e:\n",
    "                errores_count += 1\n",
    "                print(f\"Error al procesar la línea: {e}\")\n",
    "\n",
    "    print(f\"Total de errores de JSONDecodeError detectados: {errores_count}\")\n",
    "\n",
    "# crearmos el DataFrame\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna 'playtime_2weeks' porque no nos aporta informacion necesaria\n",
    "df.drop('playtime_2weeks', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna 'user_url' porque no nos aporta informacion necesaria\n",
    "df.drop('user_url', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna 'item_name' porque no nos aporta informacion necesaria\n",
    "df.drop('item_name', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el df resultante en formato parquet \n",
    "df.to_parquet('./Datasets/user_items.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
