{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv \n",
    "from textblob import TextBlob\n",
    "from ast import literal_eval\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Dataset Australian_User_reviews.json\n",
    "    Contiene datos de las recomendaciones y comentarios de los jugadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cargamos los datos desde la carpeta Datasets\n",
    "##Obserbamos que el Dataset tiene columnas con datos anidados en forma de diccionarios por lo que\n",
    "##deberemos tratarlo linea por linea\n",
    "\n",
    "# Creamos esta funcion para corregir y cargar cada linea del archivo 'australian_user_reviews.json' original\n",
    "def cargar_y_corregir_linea(linea):\n",
    "    # reemplaza las comillas simples con dobles y corrige valores booleanos\n",
    "    linea_corregida = linea.replace(\"'\", '\"').replace('True', 'true').replace('False', 'false')\n",
    "    return json.loads(linea_corregida)\n",
    "\n",
    "# lista para almacenar los datos desanidados\n",
    "data_list = []\n",
    "\n",
    "# extraemos números de la columna 'funny'\n",
    "numero_regex = re.compile(r'\\d+')\n",
    "\n",
    "# lee el archivo 'australian_user_reviews.json' original, corrige y procesa cada línea\n",
    "with open('./Datasets/australian_user_reviews.json', 'r', encoding='utf-8') as archivo_json:\n",
    "    for linea in archivo_json:\n",
    "        try:\n",
    "            entrada = cargar_y_corregir_linea(linea)\n",
    "            user_id = entrada['user_id']\n",
    "            user_url = entrada['user_url']\n",
    "            # iteramos sobre cada reseña\n",
    "            for reseña in entrada['reviews']:\n",
    "                # estraemos el numero de la columna 'funny'\n",
    "                funny_valor = re.search(numero_regex, reseña.get('funny', ''))\n",
    "                if funny_valor:\n",
    "                    funny = int(funny_valor.group())\n",
    "                else:\n",
    "                    funny = None\n",
    "                \n",
    "                # elimina 'Posted' de la columna 'posted'\n",
    "                posted = reseña.get('posted', '').replace('Posted ', '', 1) \n",
    "\n",
    "                reseña_dict = {\n",
    "                    'user_id': user_id,\n",
    "                    'user_url': user_url,\n",
    "                    'funny': funny,\n",
    "                    'posted': posted,\n",
    "                    'item_id': int(reseña.get('item_id', '')),  # convierte a entero\n",
    "                    'helpful': reseña.get('helpful', ''),\n",
    "                    'recommend': reseña.get('recommend', ''),\n",
    "                    'review': reseña.get('review', '')  # mantiene el texto original de la review\n",
    "                }\n",
    "                data_list.append(reseña_dict)\n",
    "        except json.JSONDecodeError as e:\n",
    "            None\n",
    "\n",
    "# crea el DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplaza los valores NaN por None en el DataFrame\n",
    "df = df.where(df.notna(), None)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La columna ['posted'] presenta formato de fecha completo y algunas sin el año, \n",
    "# deduzco que el año corresponde al año 2016 en curzo en el momento de la extraccion de los datos\n",
    "#por lo que realizaremos la extraccion, transformacion y asignacion del año correcto \n",
    "\n",
    "# Definimos una función para extraer y convertir la fecha\n",
    "def extract_date(posted_str):\n",
    "    if not isinstance(posted_str, str):\n",
    "        return None\n",
    "    \n",
    "    # Elimina el punto y espacios en blanco al principio y al final\n",
    "    date_str = posted_str.replace('.', '').strip()\n",
    "    try:\n",
    "        # Intenta convertir la cadena de fecha en un objeto de fecha utilizando datetime.strptime\n",
    "        date_obj = datetime.strptime(date_str, '%B %d, %Y').date()\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # Intenta nuevamente con el formato '%B %d' si la conversión falla\n",
    "            date_obj = datetime.strptime(date_str, '%B %d').date()\n",
    "            # Si solo tiene mes y día y no se proporcionó el año, asumimos el año 2016\n",
    "            date_obj = date_obj.replace(year=2016)\n",
    "        except ValueError:\n",
    "            # Si no se puede convertir a ninguna de las dos formas, devuelve None\n",
    "            date_obj = None\n",
    "\n",
    "    return date_obj\n",
    "\n",
    "# Aplica la función extract_date a la columna 'posted'\n",
    "df['posted'] = df['posted'].apply(extract_date)\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna 'user_url' porque no nos aporta informacion necesaria\n",
    "df.drop('user_url', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna 'helpful' porque no nos aporta informacion necesaria\n",
    "df.drop('helpful', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna 'funny' porque no nos aporta informacion necesaria\n",
    "df.drop('funny', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a- Analisis de Sentimientos\n",
    "    realizamos el analisis y adjuntamos la nueva columna al Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos una funcion que nos permita analizar el sentimiento de las reseñas junto con su \n",
    "# recomendación. Esta función evaluará si una reseña es positiva, neutral o negativa, \n",
    "# considerando también si la recomendación asociada es verdadera (positiva) o falsa (negativa). \n",
    "# Luego, aplicaremos esta función a cada reseña en nuestro DataFrame para obtener \n",
    "# un análisis de sentimiento más completo.\n",
    "\n",
    "def get_sentiment_with_recommend(recommend, review):\n",
    "    if isinstance(review, str):  # Verifica si el tipo de review es string\n",
    "        if recommend:  # Verifica si la recomendación es verdadera\n",
    "            analysis = TextBlob(str(review))  # Convierte a cadena y analiza el texto de la reseña\n",
    "            polarity = analysis.sentiment.polarity\n",
    "            if polarity > 0:\n",
    "                return 2  # Positivo si polaridad > 0\n",
    "            else:\n",
    "                return 1  # Neutral si polaridad <= 0 y recommend true\n",
    "        else:\n",
    "            return 0  # No recomendado, se asume neutral o negativo\n",
    "    else:\n",
    "        return 1  # No hay reseña, se asume neutral\n",
    "\n",
    "# Convertimos la columna 'review' a cadena (str)\n",
    "df['review'] = df['review'].astype(str)\n",
    "\n",
    "# Aplicamos la función a cada review\n",
    "df['sentiment_analysis'] = df.apply(lambda x: get_sentiment_with_recommend(x['recommend'], x['review']), axis=1)\n",
    "\n",
    "# Reemplazamos la columna 'review' con la nueva columna 'sentiment_analysis'\n",
    "df.drop('review', axis=1, inplace=True)\n",
    "# Renombramos a la columna 'sentiment_analysis' como 'review' \n",
    "df.rename(columns={'sentiment_analysis': 'review'}, inplace=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el df resultante en formato parquet \n",
    "df.to_parquet('./Datasets/user_reviews.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Dataset Australian_User_items.json\n",
    "    Contiene datos de Usuarios como Items y Tiempo de Juego "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cargamos los datos desde la carpeta Datasets\n",
    "##Obserbamos que el Dataset tiene columnas con datos anidados en forma de diccionarios por lo que\n",
    "##deberemos tratarlo linea por linea\n",
    "## Las Lineas con Errores de formato no se cargan en el df (55010 lineas)\n",
    "\n",
    "# Lista para almacenar los datos desanidados\n",
    "data_list = []\n",
    "\n",
    "# Función para corregir y cargar cada línea del archivo 'australian_users_items.json' original\n",
    "def cargar_y_corregir_linea(linea):\n",
    "    # reemplaza las comillas simples con dobles\n",
    "    linea_corregida = linea.replace(\"'\", '\"')\n",
    "    return json.loads(linea_corregida)\n",
    "\n",
    "# Crea y abre un archivo 'australian_users_items.csv' para escribir los datos\n",
    "with open('australian_users_items.csv', 'w', newline='', encoding='utf-8') as archivo_csv:\n",
    "    writer = csv.writer(archivo_csv)\n",
    "    \n",
    "    # Escribimos los nombres de las columnas\n",
    "    writer.writerow(['user_id', 'user_url', 'item_id', 'item_name', 'playtime_forever', 'playtime_2weeks'])\n",
    "    \n",
    "    # Inicializamos el contador de errores\n",
    "    errores_count = 0\n",
    "\n",
    "    # Lee el archivo 'australian_users_items.json' original, corrige y procesa cada línea\n",
    "    with open('./Datasets/australian_users_items.json', 'r', encoding='utf-8') as archivo_json:\n",
    "        for linea in archivo_json:\n",
    "            try:\n",
    "                entrada = cargar_y_corregir_linea(linea)\n",
    "                user_id = entrada['user_id']\n",
    "                user_url = entrada['user_url']\n",
    "                # iteracion sobre cada items\n",
    "                for articulo in entrada['items']:\n",
    "                    writer.writerow([\n",
    "                        user_id,\n",
    "                        user_url,\n",
    "                        articulo.get('item_id', ''),\n",
    "                        articulo.get('item_name', ''),\n",
    "                        articulo.get('playtime_forever', ''),\n",
    "                        articulo.get('playtime_2weeks', ''),\n",
    "                    ])\n",
    "                    data_list.append({\n",
    "                        'user_id': user_id,\n",
    "                        'user_url': user_url,\n",
    "                        'item_id': articulo.get('item_id', ''),\n",
    "                        'item_name': articulo.get('item_name', ''),\n",
    "                        'playtime_forever': articulo.get('playtime_forever', ''),\n",
    "                        'playtime_2weeks': articulo.get('playtime_2weeks', ''),\n",
    "                    })\n",
    "            except json.JSONDecodeError as e:\n",
    "                errores_count += 1\n",
    "                print(f\"Error al procesar la línea: {e}\")\n",
    "\n",
    "    print(f\"Total de errores de JSONDecodeError detectados: {errores_count}\")\n",
    "\n",
    "# crearmos el DataFrame\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna 'playtime_2weeks' porque no nos aporta informacion necesaria\n",
    "df.drop('playtime_2weeks', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna 'user_url' porque no nos aporta informacion necesaria\n",
    "df.drop('user_url', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos la columna 'item_name' porque no nos aporta informacion necesaria\n",
    "df.drop('item_name', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el df resultante en formato parquet \n",
    "df.to_parquet('./Datasets/user_items.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Dataset Output_Steam_games.json\n",
    "    Contiene las descripciones de los juegos de la plataforma\n",
    "    como desarrollador, año de lanzamiento y precio, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cargamos los datos desde la carpeta Datasets\n",
    "\n",
    "# Lista para almacenar los diccionarios convertidos\n",
    "data_list = []\n",
    "\n",
    "# Leer el archivo JSON línea por línea y convertir cada línea en un diccionario\n",
    "with open('./Datasets/output_steam_games.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Convertir la línea en un diccionario usando json.loads\n",
    "        data_dict = json.loads(line)\n",
    "        # Añadir el diccionario a la lista\n",
    "        data_list.append(data_dict)\n",
    "\n",
    "# Convertir la lista de diccionarios en un dataframe\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Verificar el contenido de la columna 'genre'\n",
    "print(df['genres'].head())\n",
    "\n",
    "# Convertir la lista de géneros en una sola línea de texto\n",
    "df['genres'] = df['genres'].apply(lambda x: ', '.join(map(str, x)) if isinstance(x, list) else x)\n",
    "\n",
    "\n",
    "# Mostrar el dataframe resultante\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={'id': 'item_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las columnas que no nos aportan informacion necesaria\n",
    "columnas_a_eliminar = ['app_name', 'title','tags', 'url', 'reviews_url', 'specs']\n",
    "\n",
    "# Eliminamos las columnas\n",
    "df = df.drop(columns=columnas_a_eliminar)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para convertir los valores de 'price' a float\n",
    "def convertir_price(valor):\n",
    "    if valor is None:\n",
    "        return 0.0  # Asigna 0.0 a los valores None\n",
    "    try:\n",
    "        return float(valor)\n",
    "    except ValueError:\n",
    "        return 0.0  # Asigna 0.0 a los valores no numéricos\n",
    "\n",
    "# Aplicar la función a la columna 'price'\n",
    "df['price'] = df['price'].apply(convertir_price)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos la columna 'release_date' a datetime con manejo de errores\n",
    "df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')\n",
    "\n",
    "# Extraemos el año y sobrescribimos la columna 'release_date' con estos valores, y luego convertimos a int\n",
    "df['release_date'] = df['release_date'].dt.year.astype('Int64')\n",
    "\n",
    "\n",
    "# Mostrar el DataFrame para verificar la conversión\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos filas con datos faltantes\n",
    "df=df.dropna(thresh=3)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el df resultante en formato parquet \n",
    "df.to_parquet('./Datasets/steam_games.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Combinaciones de Tablas para las Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a-Dataset Developer\n",
    "    Contiene nombre de los Desarrolladores, fecha de lazamiento y precio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionamos las columnas necesarias del df Steam_games\n",
    "developer = df[['publisher', 'release_date', 'price']]\n",
    "\n",
    "#Guardamos el df resultante en formato parquet \n",
    "developer.to_parquet('./Datasets/def_developer.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b- Dataset Userdata\n",
    "     Contiene el precio y recomendaciones de los usuarios por juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Para cumplir con los datos requeridos por la funcion necesitamos combinar \n",
    "## Steam_games con User_reviews usando item_id como conector \n",
    "\n",
    "# Cargamos los archivos Parquet\n",
    "df_steam_gamer = pd.read_parquet('./Datasets/steam_games.parquet')\n",
    "df_user_review = pd.read_parquet('./Datasets/user_reviews.parquet')\n",
    "\n",
    "# Verificamos los nombres de las columnas\n",
    "print(\"Columnas en df_steam_gamer:\")\n",
    "print(df_steam_gamer.columns)\n",
    "\n",
    "print(\"Columnas en df_user_review:\")\n",
    "print(df_user_review.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos 'item_id' a string en ambos DataFrames\n",
    "df_steam_gamer['item_id'] = df_steam_gamer['item_id'].astype(str)\n",
    "df_user_review['item_id'] = df_user_review['item_id'].astype(str)\n",
    "\n",
    "\n",
    "# Seleccionamos las columnas necesarias de cada DataFrame\n",
    "df_steam_gamer = df_steam_gamer[['price', 'item_id']]\n",
    "df_user_review = df_user_review[['user_id', 'item_id', 'recommend']]\n",
    "\n",
    "# Realizamos la unión usando 'item_id' como clave\n",
    "df_combined = pd.merge(df_steam_gamer, df_user_review, on='item_id', how='inner')\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(df_combined.info())\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame combinado en un nuevo archivo Parquet\n",
    "df_combined.to_parquet('./Datasets/def_userdata.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Dataset UserForGenre\n",
    "      Contiene el tiempo de juego de los usuarios por año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Para cumplir con los datos requeridos por la funcion necesitamos combinar \n",
    "## Steam_games con User_items usando item_id como conector \n",
    "\n",
    "# Cargamos los archivos Parquet\n",
    "df_steam_gamer = pd.read_parquet('./Datasets/steam_games.parquet')\n",
    "df_user_items = pd.read_parquet('./Datasets/user_items.parquet')\n",
    "\n",
    "# Verificamos los nombres de las columnas\n",
    "print(\"Columnas en df_steam_gamer:\")\n",
    "print(df_steam_gamer.columns)\n",
    "\n",
    "print(\"Columnas en df_user_items:\")\n",
    "print(df_user_items.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir 'item_id' a string en ambos DataFrames\n",
    "df_steam_gamer['item_id'] = df_steam_gamer['item_id'].astype(str)\n",
    "df_user_items['item_id'] = df_user_items['item_id'].astype(str)\n",
    "\n",
    "\n",
    "# Seleccionar las columnas necesarias de df_steam_gamer, excluyendo 'genres'\n",
    "df_steam_gamer_reduced = df_steam_gamer[['release_date', 'item_id', 'genres']]\n",
    "\n",
    "# Realizar la unión usando 'item_id' como clave\n",
    "df_combined = pd.merge(df_user_items, df_steam_gamer_reduced, on='item_id', how='inner')\n",
    "\n",
    "# Comprobar el DataFrame resultante\n",
    "print(\"Información del DataFrame combinado:\")\n",
    "print(df_combined.info())\n",
    "print(\"Primeras filas del DataFrame combinado:\")\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame combinado en un nuevo archivo Parquet\n",
    "df_combined.to_parquet('./Datasets/def_userforgenre.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
